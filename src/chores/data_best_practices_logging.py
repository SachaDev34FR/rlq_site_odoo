import pandas as pd
import numpy as np
from loguru import logger
from icecream import ic
import sys
from typing import Union, List, Optional, Tuple
import warnings
from pathlib import Path
import time

# ================================================================================
# CONFIGURATION LOGGING AVEC LOGURU
# ================================================================================

# Supprimer le handler par d√©faut de loguru
logger.remove()

# Configuration personnalis√©e de loguru
logger.add(
    sys.stderr,  # Sortie console
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="DEBUG",
    colorize=True
)

# Fichier de log pour les erreurs critiques
logger.add(
    "logs/data_processing_errors.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} - {message}",
    level="ERROR",
    rotation="10 MB",  # Rotation tous les 10MB
    retention="30 days",  # Garde 30 jours
    compression="zip"  # Compression des anciens logs
)

# Fichier de log pour tout le processus
logger.add(
    "logs/data_processing_full.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} - {message}",
    level="INFO",
    rotation="1 day",
    retention="7 days"
)

# Configuration d'icecream
ic.configureOutput(prefix='üîç DEBUG | ', includeContext=True)

# ================================================================================
# FONCTIONS UTILITAIRES DE VALIDATION ET LOGGING
# ================================================================================

def log_dataframe_info(df: pd.DataFrame, step_name: str) -> None:
    """
    Log des informations d√©taill√©es sur un DataFrame
    
    Args:
        df: DataFrame √† analyser
        step_name: Nom de l'√©tape pour identifier dans les logs
    """
    logger.info(f"=== ANALYSE DATAFRAME - {step_name.upper()} ===")
    logger.info(f"Forme: {df.shape} (lignes: {df.shape[0]}, colonnes: {df.shape[1]})")
    logger.info(f"M√©moire utilis√©e: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # Types de donn√©es
    types_info = df.dtypes.value_counts().to_dict()
    logger.info(f"Types de donn√©es: {types_info}")
    
    # Valeurs manquantes
    missing_info = df.isnull().sum()
    if missing_info.sum() > 0:
        logger.warning(f"Valeurs manquantes d√©tect√©es:")
        for col, count in missing_info[missing_info > 0].items():
            pct = (count / len(df)) * 100
            logger.warning(f"  - {col}: {count} ({pct:.2f}%)")
    else:
        logger.info("Aucune valeur manquante")
    
    # Doublons
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        logger.warning(f"Doublons d√©tect√©s: {duplicates} lignes")
    else:
        logger.info("Aucun doublon d√©tect√©")

def validate_list_column(df: pd.DataFrame, column_name: str) -> Tuple[bool, List[str]]:
    """
    Valide qu'une colonne contient bien des listes et retourne les probl√®mes d√©tect√©s
    
    Args:
        df: DataFrame √† valider
        column_name: Nom de la colonne √† v√©rifier
    
    Returns:
        Tuple[bool, List[str]]: (is_valid, liste_des_erreurs)
    """
    errors = []
    
    # V√©rification de l'existence de la colonne
    if column_name not in df.columns:
        errors.append(f"Colonne '{column_name}' introuvable")
        return False, errors
    
    # V√©rification du type des √©l√©ments
    non_list_count = 0
    empty_list_count = 0
    length_variations = []
    
    for idx, value in enumerate(df[column_name]):
        if pd.isna(value):
            errors.append(f"Valeur NaN √† l'index {idx}")
        elif not isinstance(value, list):
            non_list_count += 1
            if non_list_count <= 5:  # Limite les exemples dans les logs
                errors.append(f"Type incorrect √† l'index {idx}: {type(value)} - {value}")
        else:
            if len(value) == 0:
                empty_list_count += 1
            length_variations.append(len(value))
    
    # R√©sum√© des probl√®mes
    if non_list_count > 5:
        errors.append(f"... et {non_list_count - 5} autres valeurs de type incorrect")
    
    if empty_list_count > 0:
        errors.append(f"{empty_list_count} listes vides d√©tect√©es")
    
    # V√©rification de la coh√©rence des longueurs
    if length_variations:
        min_len, max_len = min(length_variations), max(length_variations)
        if min_len != max_len:
            logger.warning(f"Longueurs variables d√©tect√©es: min={min_len}, max={max_len}")
            ic(min_len, max_len, np.mean(length_variations))
    
    is_valid = len(errors) == 0
    return is_valid, errors

def safe_explode_column(df: pd.DataFrame, column_name: str, 
                       prefix: str = "col", validate: bool = True) -> pd.DataFrame:
    """
    √âclate une colonne de listes de mani√®re s√©curis√©e avec logging complet
    
    Args:
        df: DataFrame source
        column_name: Nom de la colonne √† √©clater
        prefix: Pr√©fixe pour les nouvelles colonnes
        validate: Si True, valide les donn√©es avant traitement
    
    Returns:
        pd.DataFrame: DataFrame avec colonnes √©clat√©es
    """
    logger.info(f"D√©but de l'√©clatement de la colonne '{column_name}'")
    start_time = time.time()
    
    try:
        # √âtape 1: Validation des donn√©es
        if validate:
            logger.info("Validation des donn√©es...")
            is_valid, errors = validate_list_column(df, column_name)
            
            if not is_valid:
                logger.error("Erreurs de validation d√©tect√©es:")
                for error in errors:
                    logger.error(f"  - {error}")
                raise ValueError(f"Validation √©chou√©e pour la colonne '{column_name}'")
            else:
                logger.success("Validation r√©ussie")
        
        # √âtape 2: Analyse des donn√©es
        log_dataframe_info(df, "avant_eclatement")
        
        # √âtape 3: Traitement des listes
        logger.info("Analyse des longueurs de listes...")
        lengths = df[column_name].apply(lambda x: len(x) if isinstance(x, list) else 0)
        max_length = lengths.max()
        unique_lengths = lengths.unique()
        
        ic(max_length, unique_lengths, lengths.describe())
        
        # √âtape 4: Normalisation si n√©cessaire
        if len(unique_lengths) > 1:
            logger.warning("Longueurs variables d√©tect√©es - normalisation n√©cessaire")
            
            def normalize_list(lst, target_length):
                if not isinstance(lst, list):
                    return [np.nan] * target_length
                normalized = lst.copy()
                while len(normalized) < target_length:
                    normalized.append(np.nan)
                return normalized
            
            normalized_lists = df[column_name].apply(lambda x: normalize_list(x, max_length))
            logger.info(f"Normalisation effectu√©e vers {max_length} √©l√©ments")
        else:
            normalized_lists = df[column_name]
            logger.info("Aucune normalisation n√©cessaire")
        
        # √âtape 5: Cr√©ation du DataFrame √©clat√©
        logger.info("Cr√©ation des nouvelles colonnes...")
        exploded_df = pd.DataFrame(normalized_lists.tolist())
        exploded_df.columns = [f'{prefix}_{i+1}' for i in range(exploded_df.shape[1])]
        
        ic(exploded_df.shape, exploded_df.columns.tolist())
        
        # √âtape 6: Concat√©nation
        logger.info("Concat√©nation avec le DataFrame original...")
        result_df = pd.concat([df.drop(column_name, axis=1), exploded_df], axis=1)
        
        # √âtape 7: Validation finale
        log_dataframe_info(result_df, "apr√®s_eclatement")
        
        execution_time = time.time() - start_time
        logger.success(f"√âclatement termin√© avec succ√®s en {execution_time:.2f}s")
        logger.info(f"Nouvelles colonnes cr√©√©es: {list(exploded_df.columns)}")
        
        return result_df
        
    except Exception as e:
        logger.error(f"Erreur lors de l'√©clatement de la colonne: {str(e)}")
        logger.exception("Traceback complet:")
        raise

# ================================================================================
# FONCTIONS DE NETTOYAGE ET VALIDATION AVANC√âES
# ================================================================================

def detect_data_quality_issues(df: pd.DataFrame) -> dict:
    """
    D√©tecte automatiquement les probl√®mes de qualit√© des donn√©es
    
    Returns:
        dict: Dictionnaire avec les probl√®mes d√©tect√©s
    """
    logger.info("Analyse de la qualit√© des donn√©es...")
    issues = {}
    
    # 1. Valeurs manquantes
    missing = df.isnull().sum()
    if missing.sum() > 0:
        issues['missing_values'] = missing[missing > 0].to_dict()
        logger.warning(f"Valeurs manquantes: {missing.sum()} au total")
    
    # 2. Doublons
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        issues['duplicates'] = duplicates
        logger.warning(f"Doublons: {duplicates} lignes")
    
    # 3. Types de donn√©es incoh√©rents
    for col in df.columns:
        if df[col].dtype == 'object':
            unique_types = set(type(x).__name__ for x in df[col].dropna())
            if len(unique_types) > 1:
                issues[f'mixed_types_{col}'] = list(unique_types)
                logger.warning(f"Types mixtes dans '{col}': {unique_types}")
    
    # 4. Valeurs aberrantes pour les colonnes num√©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)][col]
        if len(outliers) > 0:
            issues[f'outliers_{col}'] = len(outliers)
            logger.warning(f"Valeurs aberrantes dans '{col}': {len(outliers)} valeurs")
    
    ic(issues)
    return issues

def clean_dataframe(df: pd.DataFrame, cleaning_config: dict = None) -> pd.DataFrame:
    """
    Nettoie un DataFrame selon une configuration donn√©e
    
    Args:
        df: DataFrame √† nettoyer
        cleaning_config: Configuration du nettoyage
    """
    if cleaning_config is None:
        cleaning_config = {
            'remove_duplicates': True,
            'handle_missing': 'warn',  # 'drop', 'fill', 'warn'
            'normalize_strings': True,
            'validate_types': True
        }
    
    logger.info("D√©but du nettoyage des donn√©es")
    df_clean = df.copy()
    
    # Suppression des doublons
    if cleaning_config.get('remove_duplicates', False):
        initial_shape = df_clean.shape
        df_clean = df_clean.drop_duplicates()
        if df_clean.shape[0] < initial_shape[0]:
            removed = initial_shape[0] - df_clean.shape[0]
            logger.info(f"Doublons supprim√©s: {removed} lignes")
    
    # Gestion des valeurs manquantes
    missing_strategy = cleaning_config.get('handle_missing', 'warn')
    missing_count = df_clean.isnull().sum().sum()
    
    if missing_count > 0:
        if missing_strategy == 'drop':
            df_clean = df_clean.dropna()
            logger.info(f"Lignes avec valeurs manquantes supprim√©es: {missing_count}")
        elif missing_strategy == 'warn':
            logger.warning(f"Valeurs manquantes d√©tect√©es: {missing_count}")
    
    # Normalisation des cha√Ænes de caract√®res
    if cleaning_config.get('normalize_strings', False):
        string_cols = df_clean.select_dtypes(include=['object']).columns
        for col in string_cols:
            if df_clean[col].dtype == 'object':
                df_clean[col] = df_clean[col].astype(str).str.strip().str.lower()
        logger.info(f"Normalisation des cha√Ænes effectu√©e sur {len(string_cols)} colonnes")
    
    log_dataframe_info(df_clean, "apr√®s_nettoyage")
    return df_clean

# ================================================================================
# EXEMPLE D'UTILISATION COMPL√àTE
# ================================================================================

def main_example():
    """
    Exemple complet d'utilisation avec toutes les bonnes pratiques
    """
    logger.info("=== D√âBUT DU TRAITEMENT DES DONN√âES ===")
    
    try:
        # Cr√©ation de donn√©es d'exemple avec probl√®mes volontaires
        logger.info("Cr√©ation des donn√©es d'exemple...")
        
        data = {
            'id': [1, 2, 3, 4, 5, 5],  # Doublon volontaire
            'nom': ['Alice', 'Bob', None, 'Diana', 'Eve', 'Alice'],  # Valeur manquante
            'reponses_formulaire': [
                ['Oui', 'Non', 'Peut-√™tre'],
                ['Non', 'Oui', 'Oui'],
                ['Oui', 'Oui'],  # Longueur diff√©rente
                None,  # Valeur probl√©matique
                ['Oui', 'Non', 'Non', 'Oui'],  # Longueur diff√©rente
                ['Oui', 'Non', 'Peut-√™tre']
            ]
        }
        
        df = pd.DataFrame(data)
        ic(df.head())
        
        # √âtape 1: Analyse initiale
        logger.info("Analyse initiale des donn√©es")
        log_dataframe_info(df, "donn√©es_brutes")
        
        # √âtape 2: D√©tection des probl√®mes de qualit√©
        issues = detect_data_quality_issues(df)
        
        # √âtape 3: Nettoyage pr√©liminaire
        logger.info("Nettoyage pr√©liminaire...")
        
        # Traitement sp√©cifique des valeurs None dans la colonne liste
        df_clean = df.copy()
        df_clean = df_clean.dropna(subset=['reponses_formulaire'])  # Supprimer les None
        df_clean = df_clean.drop_duplicates(subset=['id', 'nom'])  # Supprimer les doublons
        
        logger.info(f"Lignes apr√®s nettoyage: {len(df_clean)}")
        
        # √âtape 4: √âclatement s√©curis√© de la colonne
        logger.info("√âclatement de la colonne de listes...")
        df_result = safe_explode_column(
            df_clean, 
            'reponses_formulaire', 
            prefix='question',
            validate=True
        )
        
        # √âtape 5: Validation finale
        logger.info("Validation finale du r√©sultat")
        final_issues = detect_data_quality_issues(df_result)
        
        if not final_issues:
            logger.success("Aucun probl√®me de qualit√© d√©tect√© dans le r√©sultat final")
        else:
            logger.warning("Probl√®mes restants apr√®s traitement:")
            ic(final_issues)
        
        # √âtape 6: Sauvegarde avec logging
        output_path = "data_processed/result.csv"
        logger.info(f"Sauvegarde vers: {output_path}")
        
        # Cr√©ation du r√©pertoire si n√©cessaire
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        df_result.to_csv(output_path, index=False)
        logger.success(f"Donn√©es sauvegard√©es: {df_result.shape[0]} lignes, {df_result.shape[1]} colonnes")
        
        return df_result
        
    except Exception as e:
        logger.critical(f"Erreur critique dans le traitement: {str(e)}")
        logger.exception("Traceback complet:")
        raise
    
    finally:
        logger.info("=== FIN DU TRAITEMENT ===")

# ================================================================================
# BONNES PRATIQUES ET POINTS D'ATTENTION
# ================================================================================

def print_best_practices():
    """
    Affiche les bonnes pratiques pour la manipulation de donn√©es
    """
    best_practices = """
    ================================================================================
    üî• BONNES PRATIQUES POUR LA MANIPULATION DE DONN√âES
    ================================================================================
    
    1. üìä VALIDATION DES DONN√âES
       ‚úÖ Toujours v√©rifier la structure et les types avant traitement
       ‚úÖ Identifier les valeurs manquantes et aberrantes
       ‚úÖ Documenter les hypoth√®ses sur les donn√©es
    
    2. üîç LOGGING ET DEBUGGING
       ‚úÖ Logger chaque √©tape importante du traitement
       ‚úÖ Utiliser icecream (ic) pour le debug rapide
       ‚úÖ Conserver les traces d'erreurs avec traceback complet
       ‚úÖ S√©parer les logs par niveau (DEBUG, INFO, WARNING, ERROR)
    
    3. üõ°Ô∏è GESTION D'ERREURS
       ‚úÖ Encapsuler le code dans des try-catch appropri√©s
       ‚úÖ Valider les entr√©es de fonctions
       ‚úÖ Pr√©voir des strat√©gies de r√©cup√©ration d'erreur
       ‚úÖ Ne jamais ignorer silencieusement les erreurs
    
    4. üíæ GESTION M√âMOIRE
       ‚úÖ Surveiller l'usage m√©moire des DataFrames volumineux
       ‚úÖ Utiliser les types de donn√©es optimaux (category, int32, etc.)
       ‚úÖ Lib√©rer la m√©moire des variables temporaires
       ‚úÖ Consid√©rer le traitement par chunks pour les gros volumes
    
    5. üîÑ REPRODUCTIBILIT√â
       ‚úÖ Fixer les seeds pour les op√©rations al√©atoires
       ‚úÖ Versionner les scripts et les donn√©es
       ‚úÖ Documenter les transformations appliqu√©es
       ‚úÖ Sauvegarder les donn√©es interm√©diaires critiques
    
    6. üß™ TESTS ET VALIDATION
       ‚úÖ Tester avec des jeux de donn√©es vari√©s
       ‚úÖ Valider les r√©sultats avec des √©chantillons connus
       ‚úÖ Impl√©menter des tests unitaires pour les fonctions critiques
       ‚úÖ V√©rifier la coh√©rence avant/apr√®s transformation
    
    ================================================================================
    ‚ö†Ô∏è  POINTS D'ATTENTION CRITIQUES
    ================================================================================
    
    üö® √âCLATEMENT DE COLONNES LISTES:
       - V√©rifier que tous les √©l√©ments sont bien des listes
       - G√©rer les listes de longueurs variables
       - Pr√©voir la normalisation avec des valeurs NaN
       - Valider la coh√©rence des donn√©es r√©sultantes
    
    üö® PERFORMANCE:
       - Les op√©rations sur DataFrames peuvent √™tre co√ªteuses
       - Pr√©f√©rer les op√©rations vectoris√©es aux boucles
       - Surveiller l'usage m√©moire avec memory_profiler si n√©cessaire
       - Utiliser des formats optimis√©s (parquet) pour les gros volumes
    
    üö® QUALIT√â DES DONN√âES:
       - Ne jamais supposer que les donn√©es sont propres
       - Toujours valider les types et formats attendus
       - Documenter les r√®gles de nettoyage appliqu√©es
       - Conserver une trace des donn√©es supprim√©es/modifi√©es
    """
    
    print(best_practices)
    logger.info("Bonnes pratiques affich√©es")

# ================================================================================
# EX√âCUTION DE L'EXEMPLE
# ================================================================================

if __name__ == "__main__":
    # Cr√©er les r√©pertoires de logs s'ils n'existent pas
    Path("logs").mkdir(exist_ok=True)
    Path("data_processed").mkdir(exist_ok=True)
    
    # Afficher les bonnes pratiques
    print_best_practices()
    
    # Ex√©cuter l'exemple complet
    result_df = main_example()
    
    # Affichage final avec icecream
    ic(result_df.head())
    ic(result_df.info())
